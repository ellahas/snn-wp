{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNN WP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "import snntorch.functional as SF\n",
    "\n",
    "from snntorch import spikeplot as splt\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn \n",
    "\n",
    "import weight_perturbation as wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a network\n",
    "\n",
    "\n",
    "class SNN_WP(nn.Module):\n",
    "    \"\"\"\n",
    "    SNN for weight perturbation. Consists of two fully connected layers of LIFs.\n",
    "    \"\"\"\n",
    "    def __init__(self, beta, num_inputs, num_hidden, num_outputs, loss):\n",
    "        \"\"\"\n",
    "        Initialize the network\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        beta : float\n",
    "            The memory leakage of the LIF\n",
    "        num_inputs : int\n",
    "            The size of the input layer\n",
    "        num_hidden : int\n",
    "            The size of the output layer\n",
    "        num_outputs : int\n",
    "            The size of the output\n",
    "        loss : Loss\n",
    "            The loss used during training\n",
    "        \"\"\"\n",
    "        super(SNN_WP, self).__init__()\n",
    "\n",
    "        spike_grad = snn.surrogate.fast_sigmoid(slope=25) #needed for BP\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.loss = loss\n",
    "\n",
    "    def clean_forward(self, x):\n",
    "        \"\"\"Wrapper function of forward for readability purposes\"\"\"\n",
    "        return self.forward(x)\n",
    "\n",
    "    def noisy_forward(self, x, noise):\n",
    "        \"\"\"Perturbs the weights and runs a forward pass\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Array\n",
    "            Input data as spike trains\n",
    "        noise : dict\n",
    "            Noise for each network parameter\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        spk : Array\n",
    "            Output spike train\n",
    "        \"\"\"\n",
    "        original_state = self.state_dict()\n",
    "        perturbed_params = wp.dictionary_add(original_state, noise)\n",
    "        self.load_state_dict(perturbed_params) \n",
    "        spk = self.forward(x)\n",
    "\n",
    "        # reset the parameters back to the unperturbed parameters\n",
    "        self.load_state_dict(original_state)\n",
    "\n",
    "        return spk\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perturbs the weights and runs a forward pass\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Array\n",
    "            Input data as spike trains\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        spk : Array\n",
    "            Output spike train\n",
    "        \"\"\"\n",
    "        self.mem1 = self.lif1.init_leaky()\n",
    "        self.mem2 = self.lif2.init_leaky()\n",
    "        for step in range(x.shape[0]):  \n",
    "            cur1 = self.fc1(x[step])  # post-synaptic current <-- spk_in x weight\n",
    "            spk1, self.mem1 = self.lif1(cur1, self.mem1)  # mem[t+1] <--post-syn current + decayed membrane\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, self.mem2 = self.lif2(cur2, self.mem2)\n",
    "        return spk2\n",
    "\n",
    "    def forward_pass(self, x, y, noise=None):\n",
    "        \"\"\"Perturbs the weights and runs a forward pass\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Array\n",
    "            Input data as spike trains\n",
    "        y : Array\n",
    "            True labels\n",
    "        noise : dict, optional\n",
    "            Noise for each network parameter\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : long\n",
    "            The loss of the pass\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            y_pred = self.clean_forward(x)\n",
    "            return self.loss(y_pred, y)\n",
    "        else:\n",
    "            y_pred = self.noisy_forward(x, noise)\n",
    "            return self.loss(y_pred, y)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Classification dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, num_samples, timesteps, dim_in):\n",
    "        \"\"\"Linear relation between input and output\"\"\"\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of samples.\"\"\"\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[:, idx, :], self.labels[:, idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ClassificationDataset(10000, 100, 8)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [9000, 1000])\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set, batch_size=100, drop_last=True\n",
    ")\n",
    "val_set = torch.utils.data.DataLoader(dataset=val_set, batch_size=100, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer parameters\n",
    "num_steps = 20\n",
    "num_inputs = 5\n",
    "num_hidden = 20\n",
    "num_outputs = 2\n",
    "beta = 0.99\n",
    "loss = SF.ce_rate_loss()\n",
    "SNN = SNN_WP(beta, num_inputs, num_hidden, num_outputs, loss)\n",
    "\n",
    "\n",
    "# training parameters\n",
    "loss_hist_wp = []\n",
    "test_acc_hist_wp = []\n",
    "loss_hist_bp = []\n",
    "test_acc_hist_bp = []\n",
    "epochs = 5\n",
    "method = \"cfd\"\n",
    "sigma = 1\n",
    "lr = 1e-6\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(test_loader, net):\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        acc = 0\n",
    "        net.eval()\n",
    "\n",
    "        test_loader = iter(test_loader)\n",
    "        for data, targets in test_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            spk_rec = net(data)\n",
    "\n",
    "            acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "            total += spk_rec.size(1)\n",
    "\n",
    "    return acc / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for WP\n",
    "\n",
    "sampler = torch.distributions.Normal(0, sigma)\n",
    "# make a sampler\n",
    "\n",
    "with torch.no_grad():\n",
    "    for e in range(epochs):\n",
    "        # get input and targets from task\n",
    "        loss_epoch = []\n",
    "        for data, targets in iter(train_loader):\n",
    "\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            params = SNN.state_dict()\n",
    "\n",
    "            loss = SNN.forward_pass(data, targets)\n",
    "            loss_hist_wp.append(loss.item())\n",
    "\n",
    "            wp_grad = wp.compute_snn_gradient(\n",
    "                SNN.forward_pass, data, targets, params, sampler, method\n",
    "            )  # do forward passes and compute gradient\n",
    "\n",
    "            new_weights = wp.update_weights(wp_grad, params, sigma, lr)\n",
    "            new_weights = wp.dictionary_mult(new_weights, 2)\n",
    "\n",
    "            SNN.load_state_dict(new_weights)  # update the weights. Huzzah\n",
    "\n",
    "            loss_epoch.append(loss.item())\n",
    "\n",
    "        loss_hist_wp.append(torch.mean(torch.tensor(loss_epoch)))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Test set forward pass\n",
    "            test_acc = batch_accuracy(test_loader, SNN)\n",
    "            print(f\"Epoch {e}, Test Acc: {test_acc * 100:.2f}%\\n\")\n",
    "            test_acc_hist_wp.append(test_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for BP\n",
    "\n",
    "optimizer = torch.optim.sgd(lr=lr)\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    loss_epoch = []\n",
    "\n",
    "    for data, targets in iter(train_loader):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        SNN.train()\n",
    "\n",
    "        y_pred = SNN(data)\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = SNN.loss(y_pred, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_epoch.append(loss_val.item())\n",
    "\n",
    "    loss_hist_bp.append(torch.mean(torch.tensor(loss_epoch)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Test set forward pass\n",
    "        test_acc = batch_accuracy(test_loader, SNN)\n",
    "        print(f\"Epoch {e}, Test Acc: {test_acc * 100:.2f}%\\n\")\n",
    "        test_acc_hist_bp.append(test_acc.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
